{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lake model continued\n",
    "\n",
    "In the previous week you used the lake problem as a means of getting aquinted with the workbench. In this assignment we will continue with the lake problem, focussing explicitly on using it for open exploration. You can use the second part of [this tutorial](https://emaworkbench.readthedocs.io/en/latest/indepth_tutorial/open-exploration.html) for help.\n",
    "\n",
    "**It is paramount that you are using the lake problem with 100 decision variables, rather than the one found on the website with the seperate anthropogenic release decision**\n",
    "\n",
    "## Apply sensitivity analysis\n",
    "There is substantial support in the ema_workbench for global sensitivity. For this, the workbench relies on [SALib](https://salib.readthedocs.io/en/latest/) and feature scoring which is a machine learning alternative for global sensitivity analysis. \n",
    "\n",
    "\n",
    "1. Apply Sobol with 3 seperate release policies (0, 0.05, and 0.1) and analyse the results for each release policy seperately focusing on the reliability objective. Do the sensitivities change depending on the release policy? Can you explain why or why not?\n",
    "\n",
    "*hint: you can use sobol sampling for the uncertainties, and set policies to a list with the 3 different release policies. Next, for the analysis using logical indexing on the experiment.policy column you can select the results for each seperate release policy and apply sobol to each of the three seperate release policies. If this sounds too complicated, just do it on each release policy seperately.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lakemodel_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e7607c0c224d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlakemodel_function\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlake_problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRealParameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mScalarOutcome\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiprocessingEvaluator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mema_logging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperform_experiments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalysis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdimensional_stacking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lakemodel_function'"
     ]
    }
   ],
   "source": [
    "from lakemodel_function import lake_problem\n",
    "from ema_workbench import Model, RealParameter, ScalarOutcome\n",
    "from ema_workbench import MultiprocessingEvaluator, ema_logging, perform_experiments\n",
    "from ema_workbench.analysis import prim\n",
    "from ema_workbench.analysis import dimensional_stacking\n",
    "from SALib.analyze import sobol\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from ema_workbench import Policy\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "\n",
    "from ema_workbench.em_framework.evaluators import SOBOL\n",
    "from ema_workbench.em_framework import get_SALib_problem\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "mpl.rcParams['lines.markersize'] = 3\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of levers in the lake_model\n",
    "n_levers = 100\n",
    "\n",
    "model = Model('LakeProblem', function=lake_problem)\n",
    "\n",
    "# specify uncertainties\n",
    "model.uncertainties = [RealParameter('mean', 0.01, 0.05),\n",
    "                       RealParameter('stdev', 0.001, 0.005),\n",
    "                       RealParameter('b', 0.1, 0.45),\n",
    "                       RealParameter('q', 2, 4.5),\n",
    "                       RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "# specify levers\n",
    "model.levers = [RealParameter(f\"l{i}\", 0, 0.1) for i in range(n_levers)]\n",
    "\n",
    "# specify outcomes\n",
    "model.outcomes = [ScalarOutcome('max_P'),\n",
    "                  ScalarOutcome('utility'),\n",
    "                  ScalarOutcome('inertia'),\n",
    "                  ScalarOutcome('reliability')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the policies\n",
    "policies = [Policy(\"0\", **{l.name:0 for l in model.levers}),\n",
    "          Policy(\"0.05\", **{l.name:0.05 for l in model.levers}),\n",
    "          Policy(\"0.1\", **{l.name:0.1 for l in model.levers})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments \n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=100, policies=policies , uncertainty_sampling='sobol')\n",
    "\n",
    "experiments, outcomes = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the performed experiments, we now perform a Sobol sensitivity analysis and analyse differences observed between the different policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every policy, get the outcomes and analyse with sobol\n",
    "policy_names = experiments.policy.unique().tolist()\n",
    "\n",
    "problem = get_SALib_problem(model.uncertainties)\n",
    "\n",
    "n_policies = len(policies)\n",
    "n_experiments = len(experiments)\n",
    "n_exp_pol = n_experiments/n_policies\n",
    "\n",
    "sa_results = {}\n",
    "\n",
    "for i in range(1,len(policies)+1):\n",
    "    result = outcomes['reliability'][int(n_exp_pol*(i-1)): int(n_exp_pol*i)]\n",
    "    \n",
    "    sa = sobol.analyze(problem, result,\n",
    "                   calc_second_order=True, print_to_console=False)\n",
    "    sa_results[policy_names[i-1]] = sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have N = 1200 for each policy, while SOBOL requires >100(k+2) where k is the number of uncertainties. This means we won't have a strong degree of stability for the SOBOL indices. We chose which uncertainties are relevant or not and then rerun with a lower need of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_filtered = {k:sa[k] for k in ['ST','ST_conf','S1','S1_conf']}\n",
    "Si_df = pd.DataFrame(scores_filtered, index=problem['names'])\n",
    "\n",
    "#sns.set_style('white')\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "indices = Si_df[['S1','ST']]\n",
    "err = Si_df[['S1_conf','ST_conf']]\n",
    "\n",
    "indices.plot.bar(yerr=err.values.T,ax=ax)\n",
    "fig.set_size_inches(8,6)\n",
    "fig.subplots_adjust(bottom=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repeat the above analysis for the 3 release policies but now with extra trees feature scoring and for all outcomes of interest. As a bonus, use the sobol experiment results as input for extra trees, and compare the results with those resulting from latin hypercube sampling.\n",
    "\n",
    "*hint: you can use [seaborn heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html) for a nice figure of the results*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping irrelevant columns for feature selection per policy\n",
    "experiments_cleaned = experiments\n",
    "experiments_cleaned.drop(['policy', 'model'], axis=1)\n",
    "experiments_cleaned.drop(columns=[f\"l{i}\" for i in range(n_levers)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection per policy\n",
    "outcomes_names = ['max_P','utility','inertia','reliability']\n",
    "fs_results = {}\n",
    "\n",
    "for i in range(1, len(policies)+1):\n",
    "    for outcome in outcomes_names:\n",
    "        result = outcomes[outcome][int(n_exp_pol*(i-1)): int(n_exp_pol*i)]\n",
    "\n",
    "        fs_results[outcome] = result\n",
    "    \n",
    "    print('Policy: %s' % policy_names[i-1])\n",
    "    scores = feature_scoring.get_feature_scores_all(experiments_cleaned[int(n_exp_pol*(i-1)): int(n_exp_pol*i)],\n",
    "                                                   fs_results)\n",
    "    sns.heatmap(scores, annot=True, cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# story of what we see yay #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across all policies we observe that the input variable 'b' influences the max_p and reliability outcome with moderate to strong effect. Especially its effect on max_p increases when the policy levers are non-zero. Additionally, the input variable delta starts to have a strong effect on the utility outcome in the non-zero policies. ......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
